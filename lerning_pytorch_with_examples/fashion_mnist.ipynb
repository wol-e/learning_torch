{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f30d96f2",
   "metadata": {},
   "source": [
    "# Fashion MNIST with PyTorch\n",
    "\n",
    "Based on https://pytorch.org/tutorials/beginner/nn_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e49ae0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9f04c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_openml(name=\"Fashion-MNIST\", version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c17cc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.690196</td>\n",
       "      <td>0.682353</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.619608</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.423529</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.545098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.121569</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel1  pixel2  pixel3    pixel4  pixel5    pixel6  pixel7  pixel8  \\\n",
       "0         0.0     0.0     0.0  0.000000     0.0  0.000000     0.0     0.0   \n",
       "1         0.0     0.0     0.0  0.000000     0.0  0.000000     0.0     0.0   \n",
       "2         0.0     0.0     0.0  0.000000     0.0  0.000000     0.0     0.0   \n",
       "3         0.0     0.0     0.0  0.000000     0.0  0.000000     0.0     0.0   \n",
       "4         0.0     0.0     0.0  0.003922     0.0  0.003922     0.0     0.0   \n",
       "...       ...     ...     ...       ...     ...       ...     ...     ...   \n",
       "69995     0.0     0.0     0.0  0.000000     0.0  0.000000     0.0     0.0   \n",
       "69996     0.0     0.0     0.0  0.000000     0.0  0.000000     0.0     0.0   \n",
       "69997     0.0     0.0     0.0  0.000000     0.0  0.000000     0.0     0.0   \n",
       "69998     0.0     0.0     0.0  0.000000     0.0  0.000000     0.0     0.0   \n",
       "69999     0.0     0.0     0.0  0.000000     0.0  0.000000     0.0     0.0   \n",
       "\n",
       "       pixel9   pixel10  ...  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0         0.0  0.000000  ...  0.000000  0.000000       0.0  0.000000   \n",
       "1         0.0  0.003922  ...  0.000000  0.000000       0.0  0.000000   \n",
       "2         0.0  0.000000  ...  0.000000  0.000000       0.0  0.000000   \n",
       "3         0.0  0.000000  ...  0.619608  0.627451       0.0  0.423529   \n",
       "4         0.0  0.007843  ...  0.294118  0.121569       0.0  0.000000   \n",
       "...       ...       ...  ...       ...       ...       ...       ...   \n",
       "69995     0.0  0.000000  ...  0.000000  0.000000       0.0  0.000000   \n",
       "69996     0.0  0.000000  ...  0.156863  0.000000       0.0  0.000000   \n",
       "69997     0.0  0.000000  ...  0.000000  0.000000       0.0  0.000000   \n",
       "69998     0.0  0.000000  ...  0.000000  0.000000       0.0  0.000000   \n",
       "69999     0.0  0.000000  ...  0.000000  0.000000       0.0  0.000000   \n",
       "\n",
       "       pixel779  pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0      0.000000  0.000000  0.000000       0.0       0.0       0.0  \n",
       "1      0.690196  0.682353  0.137255       0.0       0.0       0.0  \n",
       "2      0.000000  0.000000  0.000000       0.0       0.0       0.0  \n",
       "3      0.862745  0.545098  0.000000       0.0       0.0       0.0  \n",
       "4      0.000000  0.000000  0.000000       0.0       0.0       0.0  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "69995  0.000000  0.000000  0.000000       0.0       0.0       0.0  \n",
       "69996  0.000000  0.000000  0.000000       0.0       0.0       0.0  \n",
       "69997  0.000000  0.000000  0.000000       0.0       0.0       0.0  \n",
       "69998  0.000000  0.000000  0.000000       0.0       0.0       0.0  \n",
       "69999  0.000000  0.000000  0.000000       0.0       0.0       0.0  \n",
       "\n",
       "[70000 rows x 784 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data[\"data\"]\n",
    "X[\"Target\"] = data[\"target\"]\n",
    "X = X.sample(frac=1).reset_index(drop=True)\n",
    "y = X[\"Target\"].copy().astype(int)\n",
    "del X[\"Target\"]\n",
    "X = X / 255\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9c7f3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.003149</td>\n",
       "      <td>0.008645</td>\n",
       "      <td>0.022098</td>\n",
       "      <td>0.056510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135617</td>\n",
       "      <td>0.091328</td>\n",
       "      <td>0.065144</td>\n",
       "      <td>0.069896</td>\n",
       "      <td>0.089757</td>\n",
       "      <td>0.070463</td>\n",
       "      <td>0.033428</td>\n",
       "      <td>0.010790</td>\n",
       "      <td>0.003281</td>\n",
       "      <td>0.000286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.004709</td>\n",
       "      <td>0.009643</td>\n",
       "      <td>0.017287</td>\n",
       "      <td>0.022911</td>\n",
       "      <td>0.032104</td>\n",
       "      <td>0.055362</td>\n",
       "      <td>0.092937</td>\n",
       "      <td>0.149758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225822</td>\n",
       "      <td>0.191903</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.172163</td>\n",
       "      <td>0.203346</td>\n",
       "      <td>0.177379</td>\n",
       "      <td>0.115796</td>\n",
       "      <td>0.068179</td>\n",
       "      <td>0.036308</td>\n",
       "      <td>0.008353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223529</td>\n",
       "      <td>0.031373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.890196</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.878431</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             pixel1        pixel2        pixel3        pixel4        pixel5  \\\n",
       "count  70000.000000  70000.000000  70000.000000  70000.000000  70000.000000   \n",
       "mean       0.000003      0.000025      0.000135      0.000388      0.000972   \n",
       "std        0.000343      0.001163      0.004709      0.009643      0.017287   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        0.062745      0.176471      0.854902      0.725490      0.890196   \n",
       "\n",
       "             pixel6        pixel7        pixel8        pixel9       pixel10  \\\n",
       "count  70000.000000  70000.000000  70000.000000  70000.000000  70000.000000   \n",
       "mean       0.001603      0.003149      0.008645      0.022098      0.056510   \n",
       "std        0.022911      0.032104      0.055362      0.092937      0.149758   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        0.901961      0.878431      0.882353      0.996078      1.000000   \n",
       "\n",
       "       ...      pixel775      pixel776      pixel777      pixel778  \\\n",
       "count  ...  70000.000000  70000.000000  70000.000000  70000.000000   \n",
       "mean   ...      0.135617      0.091328      0.065144      0.069896   \n",
       "std    ...      0.225822      0.191903      0.165000      0.172163   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    ...      0.223529      0.031373      0.000000      0.000000   \n",
       "max    ...      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "           pixel779      pixel780      pixel781      pixel782      pixel783  \\\n",
       "count  70000.000000  70000.000000  70000.000000  70000.000000  70000.000000   \n",
       "mean       0.089757      0.070463      0.033428      0.010790      0.003281   \n",
       "std        0.203346      0.177379      0.115796      0.068179      0.036308   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "           pixel784  \n",
       "count  70000.000000  \n",
       "mean       0.000286  \n",
       "std        0.008353  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        0.666667  \n",
       "\n",
       "[8 rows x 784 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1da546a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        3\n",
       "1        4\n",
       "2        7\n",
       "3        6\n",
       "4        0\n",
       "        ..\n",
       "69995    5\n",
       "69996    8\n",
       "69997    9\n",
       "69998    9\n",
       "69999    7\n",
       "Name: Target, Length: 70000, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd53d736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAVOUlEQVR4nO3dfXBc1XkG8OfZ1UqyZPkL+RsnJmADxjM4RDFJIS2UJAU6KYQ/KKZDSSepM2mYNmk6JaGThpnOpLSThCFtSmoCAZoAYSbJQFtoADcdhgnQyMTFdjAxNsb4U/gLWd/a3bd/7BWzNrrvlbV3P6Tz/GY0Wu177+7RlR/f3T33nEMzg4hMf5l6N0BEakNhFwmEwi4SCIVdJBAKu0ggFHaRQCjsIoFQ2Kcgkn1lX0WSg2U//1GN2nAZyb2nuc8XSe4i2UtyP8k7STZVq41yMoV9CjKzmWNfAPYA+ETZfT+cyGPUKWSPA7jIzGYBWA3gQgB/Xod2BElhn0ZIriX5PMnjJA+Q/GeSzWV1I/l5kjsA7Iju++to2/0kPxNtc05UayH5DZJ7SB4i+V2SM0i2A3gSwJKyVxRLktpnZjvN7PhYcwAUAZyT8mGQGAr79FIA8EUAnQA+DOAKAH92yjbXArgYwCqSVwL4SwAfRSl0l52y7R0AVgJYE9WXAvhbM+sHcBWA/WWvKPaTvJTkca+BJG8k2QvgMEpn9n+d1G8qp01hn0bMbJOZvWBmeTPbjVKQfueUzf7ezI6a2SCA6wF838y2mdkAgNvHNiJJAOsBfDHa/gSArwO4wXn+58xsTkIbH4pexq8E8F0Ah07395TJ0Ycj0wjJlQC+BaALQBtKf99Np2z2ZtntJQC6Y2rzo8fYVMp96SkAZNNoq5ntILkNwL8AuC6NxxSfzuzTy90AtgNYEZ09b0MpoOXKhzkeAHBm2c/Lym4fBjAI4AIzmxN9zY4+FDz1cSarCcDZKTyOTIDCPr10AOgF0EfyPACfS9j+UQB/QvJ8km0AvjpWMLMigHsA3ElyAQCQXEry96JNDgE4g+TsiTYu+gBw7LFWAfgKgI0T3V8qo7BPL38F4EYAJ1AK6o+8jc3sSQDfBvBzAK8BeCEqDUffbx27P/pQ7RkA50b7bgfwMIBd0af/S0h+hGSf85SXANhCsh/AE9HXbaf9W8qkUJNXyBiS5wPYCqDFzPL1bo+kS2f2wJH8ZNSfPhfAPwD4dwV9elLY5bMAegDsRKmfPul9vkxRehkvEgid2UUCUdOLaprZYq1or+VTTgmFef4xsXn+W+hif/yfsflA/6TaVAuc0erWh+b75yLmT72E4GTN+xv3d6+WIfRjxIbHPTAVhT26tvoulK6q+p6Z3eFt34p2XMwrKnnKaan3qg+59ZF1R9364IudsbVlX3/Rf/Jiwa9XUeac89z69ltmufXmt/yL+ZZ/9fnTbtNU96LFX7Yw6ZfxJLMAvoPSgIhVANZFF0qISAOq5D37WgCvmdkuMxsB8AiAa9JploikrZKwL8XJAyf2RvedhOR6kt0ku0ffuTBLRGqt6p/Gm9kGM+sys64cWqr9dCISo5Kw78PJo6TOjO4TkQZUSdh/CWAFybOiqY9uQGmOMRFpQJPuejOzPMlbAPwMpa63+8xsW2otazBsij9Ulvf7wffd+ltu/eufud+t/3ev38nxsU/F/x/70c+dcPf92YA/QnVB1t+/p9Dh1lfkDsfWXhp+M7YGANsG3vUR0EkWN7/t1p+89ILYWuHy/e6+01FF/exmNjZMUUQanC6XFQmEwi4SCIVdJBAKu0ggFHaRQCjsIoHQIhETlNSX7hl5vzfhKvBQjz/E9dhQm1vf/vbC2NpP2vy+6CTFd007f7ImFv39LX7/t4ZnxtYm4qXCMrfemo3/mw1W9MxTk87sIoFQ2EUCobCLBEJhFwmEwi4SCIVdJBDqequBVYsPufX27Ij/AP6My+7+M7Kj7r79hWa/nvfrSV1vGcYvQnJGiz/V87Fhv8uxOevPjHtm2/HY2q7zV7j7Fl7Z4danIp3ZRQKhsIsEQmEXCYTCLhIIhV0kEAq7SCAUdpFAqJ+9Bo4Pz3DrXn8wAHS2+ENkj47EL/k8XPT/xIOFnFtP6kdvb/KvEZiRdA2B4/XeM9z6rJYhv94UP5D16Af8x56tfnYRmaoUdpFAKOwigVDYRQKhsIsEQmEXCYTCLhII9bOnINPqDzif0+JPXNyU8cdld+b8ZZNf7Y2fSnpmbtjdN2m8e1K9d9T/3Xf3zYutvaf9mLvv7y/Z4ta39y1265633+ef5/yFrKemisJOcjeAEwAKAPJm1pVGo0QkfWmc2S83s8MpPI6IVJHes4sEotKwG4CnSG4iuX68DUiuJ9lNsnsU/vtHEameSl/GX2pm+0guAPA0ye1m9mz5Bma2AcAGAJjFefGzD4pIVVV0ZjezfdH3HgA/BbA2jUaJSPomHXaS7SQ7xm4D+DiArWk1TETSVcnL+IUAfkpy7HEeMrP/SqVVU0xxzUq3PjO3x617Sy4DwPPf/6Bb/7uvfS+29vixi9x9MxXM+z4ROecagus6u919b3nmj916+wJ/3vkbz4l/fFvtX7swHU067Ga2C8CFKbZFRKpIXW8igVDYRQKhsIsEQmEXCYTCLhIIDXFNwYnl/tLCH5l50K0fzPkDKnf+qsOtL23qja3tGZjr7tuaMIS1o8m/xLlvtMWtD4zGL/nckfGngp69zf/n2dvmP7c3NHhGi/97T0c6s4sEQmEXCYTCLhIIhV0kEAq7SCAUdpFAKOwigVA/ewqOXEC3vrL1gFvfOTDfrRe3bnfrC7Pxw1Sb6E9TnTSE9diIv9x0fz6+Hx0AhvLx/8R2DC9y9x1Y4rftPQ9n3fqcD8UPgV0257i773ScQE1ndpFAKOwigVDYRQKhsIsEQmEXCYTCLhIIhV0kEOpnT8HoLL8/+IJmfzz7rS/9oVtfif91653Z9thaf8J486JVNq67vWnEreeL8eeTQtK55ix/quiWr7zs1ofuzMXWzp/l/022zJrl1gu98XMINCqd2UUCobCLBEJhFwmEwi4SCIVdJBAKu0ggFHaRQKifPQXW4i97vC/v99me8St/XHaS10f7YmtF+GPtR4r+cxfN398S6gWnn31r/1J33452f175JIed4570e7HNH8eP6djPTvI+kj0kt5bdN4/k0yR3RN/9lQhEpO4m8jL+fgBXnnLflwFsNLMVADZGP4tIA0sMu5k9C+DoKXdfA+CB6PYDAK5Nt1kikrbJvmdfaGZjE6sdBLAwbkOS6wGsB4BW+GuiiUj1VPxpvJkZgNiRIGa2wcy6zKwrB39QhohUz2TDfojkYgCIvvek1yQRqYbJhv1xADdHt28G8Fg6zRGRakl8z07yYQCXAegkuRfA1wDcAeBRkp8G8AaA66vZyIbX7Pez7xpZ4Nbnbvf7k4c+sdat/3o0fl750YLfj57L+PPKN2f9+kjC47fl4se77x2Y4+578aI9bn2nWwVWt74ZW9vWt8Td12Z3+A9+8FDCszeexLCb2bqY0hUpt0VEqkiXy4oEQmEXCYTCLhIIhV0kEAq7SCA0xDUF2da8W09aFnlwob/s8b6r/O6v/+k9P7aWNJTTG4IKAIP5+OmYAaA5oetubutAbO3YkH/59Afmb3HrOy49dXzWyR7smRNba8n4fzMU/N9rKtKZXSQQCrtIIBR2kUAo7CKBUNhFAqGwiwRCYRcJhPrZU9DeNuzXM349O+T3w7932WG3fnhkZmxtZrP/3En95DNQ2ZLOmfhJjNw+eABYlDvu1o+v8Kd7bneWqz5vrr9k84HsIrc+FenMLhIIhV0kEAq7SCAUdpFAKOwigVDYRQKhsIsEQv3sKTi3018j46KW+CmNAaB/kT8d8+Wdb7j1oyPtsbU5zYPuvklLOnv95ADQkvXHhTcxfprtvPnnmo6MP8X20Qv9tl0y80hs7Q9mbXb33XjWR9x686tuuSHpzC4SCIVdJBAKu0ggFHaRQCjsIoFQ2EUCobCLBEL97CnY8fC5bn1dxq+3DvpLPt8073m3/k+H4hfU7Wzpc/dNUkzoC884/egAMDMbP56+rxA/3hwANp64wK0vW+WPSX/hzq7Y2n9evdrdd8Vr/hwCU3FW+cQzO8n7SPaQ3Fp23+0k95HcHH1dXd1mikilJvIy/n4A4y29caeZrYm+nki3WSKStsSwm9mzAI7WoC0iUkWVfEB3C8mXo5f5c+M2IrmeZDfJ7lH486GJSPVMNux3AzgbwBoABwB8M25DM9tgZl1m1pWD/4GMiFTPpMJuZofMrGBmRQD3AFibbrNEJG2TCjvJxWU/fhLA1rhtRaQxJPazk3wYwGUAOknuBfA1AJeRXAPAAOwG8NnqNbHxLfjOLyraf+C6i936f/Re6NZzztzvM7L+vO9Z+P3ko+aPtU9ae743Hz+3e3/BX5e+P++/7btp2Qtu/dEfxM/9PvsH7q5Tsh89SWLYzWzdOHffW4W2iEgV6XJZkUAo7CKBUNhFAqGwiwRCYRcJhIa4piDT1ubWiwP+0sRvL/e7tw6OzHLrw4X4P2Nzxv8TFy1hKumErrV8wW/70ZH4Y9OX0LU2kvDYHfP8abIz7fFTbBf7+919Qf+4wPzj0oh0ZhcJhMIuEgiFXSQQCrtIIBR2kUAo7CKBUNhFAqF+9hQUh6o73dZgwlDQI8Px/clJw0iThsC2Z0fc+mAh59YH8vHP30R/IOmw+f88HzmYNGeKf32DiwnnQZt6g2B1ZhcJhMIuEgiFXSQQCrtIIBR2kUAo7CKBUNhFAqF+9gZQ9LuqEw0549mTHB+On+oZSB7P3prQT19E/Ljw/tFWd9+O3JBbn93s14eWnxlf3Paqu+90pDO7SCAUdpFAKOwigVDYRQKhsIsEQmEXCYTCLhKIiSzZvAzAgwAWorRE8wYzu4vkPAA/ArAcpWWbrzezY9Vr6vSVO+HX5zf7G+zpmzvp5664H93880W+GF+f0eQ/dms279bPbnvLrb85f0VszZ+RfnqayJk9D+BLZrYKwIcAfJ7kKgBfBrDRzFYA2Bj9LCINKjHsZnbAzF6Kbp8A8AqApQCuAfBAtNkDAK6tUhtFJAWn9Z6d5HIA7wfwIoCFZnYgKh1E6WW+iDSoCYed5EwAPwbwBTPrLa+ZmaH0fn68/daT7CbZPYrqztUmIvEmFHaSOZSC/kMz+0l09yGSi6P6YgA94+1rZhvMrMvMunLwF/ITkepJDDtJArgXwCtm9q2y0uMAbo5u3wzgsfSbJyJpmcjYyEsA3ARgC8nN0X23AbgDwKMkPw3gDQDXV6WFUwAz/vK+VvT3n73b74Ja0Nzr1r0urAz9J29O+O8+b34nVVLX3PzWvtjanJw/1XPSFNqZ8d85voOFqbescjUlht3MngNiByVfkW5zRKRadAWdSCAUdpFAKOwigVDYRQKhsIsEQmEXCYSmkm4ArYf8/uZj+fglmQG/LzuX8ZcWTlpyOUlnc/xzA0Cbs+RzC/0hrEn97AdHZrl1ncpOpsMhEgiFXSQQCrtIIBR2kUAo7CKBUNhFAqGwiwRC/ewpsGJl46Yze/0pkXf2z3frS2ccj611ZP1ljUcTxqvn6PfTFxKmkj4yGn+NQH/en7noyLB/fcGStrfd+vCc+GsI/IWqpyed2UUCobCLBEJhFwmEwi4SCIVdJBAKu0ggFHaRQKifvRF0+P3JQPyYcAB4vf+M2NqCFn+8eXuTvyTXaMLixgPFhDHnQ/FjzkcK/j+/wbw/1r4p4RqAzIjmjS+nM7tIIBR2kUAo7CKBUNhFAqGwiwRCYRcJhMIuEojEfnaSywA8CGAhAAOwwczuInk7gD8FMDYY+zYze6JaDW1kla7Pjp4jbjlv/p/p7eH40dnHhtvcfRfOOOHWR4p+P/tbgzPdeqEYfz7JZvwD05RQz9DvR28+7l+fEJqJXFSTB/AlM3uJZAeATSSfjmp3mtk3qtc8EUlLYtjN7ACAA9HtEyRfAbC02g0TkXSd1nt2kssBvB/Ai9Fdt5B8meR9JOfG7LOeZDfJ7lH4l2aKSPVMOOwkZwL4MYAvmFkvgLsBnA1gDUpn/m+Ot5+ZbTCzLjPrysGfc0xEqmdCYSeZQynoPzSznwCAmR0ys4KZFQHcA2Bt9ZopIpVKDDtJArgXwCtm9q2y+xeXbfZJAFvTb56IpGUin8ZfAuAmAFtIbo7uuw3AOpJrUOqO2w3gs1VoXxgW+VNFr+7Y5tb3N8+JrWXgd0/1JyyLnOTDna+79c5c/BDbw6N+t93xvN9tuKptv1v/xbkfjK3Nfd7dtfLu1AY0kU/jnwMw3m8eZJ+6yFSlK+hEAqGwiwRCYRcJhMIuEgiFXSQQCrtIIDSVdAqs4E9pnKTwm51u/ZGHftett1xyOLY2POpPx5zL+m1nwjDSbbbIrY8W4ofIDg36ffzZJr9tTxXOc+srH9seW0v6i1X6N21EOrOLBEJhFwmEwi4SCIVdJBAKu0ggFHaRQCjsIoGgWe2WtSX5FoA3yu7qBBDfSVxfjdq2Rm0XoLZNVppte6+ZjTtBQk3D/q4nJ7vNrKtuDXA0atsatV2A2jZZtWqbXsaLBEJhFwlEvcO+oc7P72nUtjVquwC1bbJq0ra6vmcXkdqp95ldRGpEYRcJRF3CTvJKkq+SfI3kl+vRhjgkd5PcQnIzye46t+U+kj0kt5bdN4/k0yR3RN/HXWOvTm27neS+6NhtJnl1ndq2jOTPSf6a5DaSfxHdX9dj57SrJset5u/ZSWYB/AbAxwDsBfBLAOvM7Nc1bUgMkrsBdJlZ3S/AIPnbAPoAPGhmq6P7/hHAUTO7I/qPcq6Z3dogbbsdQF+9l/GOVitaXL7MOIBrAXwKdTx2TruuRw2OWz3O7GsBvGZmu8xsBMAjAK6pQzsanpk9C+DoKXdfA+CB6PYDKP1jqbmYtjUEMztgZi9Ft08AGFtmvK7HzmlXTdQj7EsBvFn281401nrvBuApkptIrq93Y8ax0MwORLcPAlhYz8aMI3EZ71o6ZZnxhjl2k1n+vFL6gO7dLjWziwBcBeDz0cvVhmSl92CN1Hc6oWW8a2WcZcbfUc9jN9nlzytVj7DvA7Cs7Oczo/sagpnti773APgpGm8p6kNjK+hG33vq3J53NNIy3uMtM44GOHb1XP68HmH/JYAVJM8i2QzgBgCP16Ed70KyPfrgBCTbAXwcjbcU9eMAbo5u3wzgsTq25SSNsox33DLjqPOxq/vy52ZW8y8AV6P0ifxOAH9TjzbEtOt9AP4v+tpW77YBeBill3WjKH228WkAZwDYCGAHgGcAzGugtv0bgC0AXkYpWIvr1LZLUXqJ/jKAzdHX1fU+dk67anLcdLmsSCD0AZ1IIBR2kUAo7CKBUNhFAqGwiwRCYRcJhMIuEoj/B2wHg3MENC2oAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_image_by_index(i):\n",
    "    plt.imshow(X.iloc[i].values.reshape(28,28))\n",
    "    plt.title(f\"Target: {y.iloc[i]}\")\n",
    "\n",
    "\n",
    "plot_image_by_index(159)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44d911a",
   "metadata": {},
   "source": [
    "# torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28cdeb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1c3791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93c650ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 3, ..., 3, 8, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "966226c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"pandas to torch\"\"\"\n",
    "\n",
    "x_train, y_train, x_test, y_test = map(\n",
    "    torch.tensor, (\n",
    "        x_train.values,\n",
    "        y_train.values,\n",
    "        x_test.values,\n",
    "        y_test.values\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c62f4229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    \"\"\"\n",
    "    activation function\n",
    "    \"\"\"\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def manual_model(xb):\n",
    "    \"\"\"\n",
    "    linear model layer with log softmax activation\n",
    "    param xb: training batch\n",
    "    \"\"\"\n",
    "    return log_softmax(xb @ weights + bias)  # @ is matrix multiplication\n",
    "\n",
    "def nll_loss(preds, target):\n",
    "    \"\"\"\n",
    "    negative log likelihood\n",
    "    \"\"\"\n",
    "    return -preds[range(target.shape[0]), target].mean()\n",
    "\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20cbe6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(2.4528, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "1 tensor(2.4641, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "2 tensor(2.2877, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "3 tensor(2.2162, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "4 tensor(2.2598, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "5 tensor(2.2631, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "6 tensor(2.1948, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "7 tensor(2.1464, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "8 tensor(2.2416, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "9 tensor(2.2116, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "10 tensor(2.1269, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "100 tensor(1.2949, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "200 tensor(1.0230, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "300 tensor(0.9081, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "400 tensor(0.8658, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "500 tensor(0.8400, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "600 tensor(0.8869, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "700 tensor(0.7937, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "0 tensor(0.8075, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "1 tensor(0.7454, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "2 tensor(0.9260, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "3 tensor(0.7499, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "4 tensor(0.7682, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "5 tensor(0.7756, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "6 tensor(0.7309, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "7 tensor(0.7600, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "8 tensor(0.8080, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "9 tensor(0.7085, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "10 tensor(0.6466, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "100 tensor(0.7283, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "200 tensor(0.7067, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "300 tensor(0.6549, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "400 tensor(0.6611, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "500 tensor(0.6823, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "600 tensor(0.7548, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "700 tensor(0.6518, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(0.7894)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "manual nn with torch tensors form scratch\n",
    "\"\"\"\n",
    "\n",
    "weights = (torch.randn(784, 10) / math.sqrt(784)).double()  # \"Xavier initialisation\"\n",
    "weights.requires_grad_()  # trailing _ in pytorch means \"inplace\"\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "lr = 0.01  # learning rate\n",
    "epochs = 2  # how many epochs to train for\n",
    "n, c = x_train.shape\n",
    "bs = 60\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = manual_model(xb)\n",
    "        loss = nll_loss(pred, yb)\n",
    "        if i % 100 == 0 or i <= 10:\n",
    "            print(i, loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "            \n",
    "print(accuracy(manual_model(x_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "469c365e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, it 0: loss 2.37\n",
      "Epoch 0, it 1: loss 2.272\n",
      "Epoch 0, it 2: loss 2.308\n",
      "Epoch 0, it 3: loss 2.325\n",
      "Epoch 0, it 4: loss 2.235\n",
      "Epoch 0, it 5: loss 2.182\n",
      "Epoch 0, it 6: loss 2.141\n",
      "Epoch 0, it 7: loss 2.146\n",
      "Epoch 0, it 8: loss 2.137\n",
      "Epoch 0, it 9: loss 2.139\n",
      "Epoch 0, it 10: loss 2.053\n",
      "Epoch 0, it 100: loss 1.332\n",
      "Epoch 0, it 200: loss 0.959\n",
      "Epoch 0, it 300: loss 1.011\n",
      "Epoch 0, it 400: loss 0.88\n",
      "Epoch 0, it 500: loss 0.774\n",
      "Epoch 0, it 600: loss 0.863\n",
      "Epoch 0, it 700: loss 0.737\n",
      "Epoch 1, it 0: loss 0.824\n",
      "Epoch 1, it 1: loss 0.717\n",
      "Epoch 1, it 2: loss 0.99\n",
      "Epoch 1, it 3: loss 0.762\n",
      "Epoch 1, it 4: loss 0.791\n",
      "Epoch 1, it 5: loss 0.697\n",
      "Epoch 1, it 6: loss 0.759\n",
      "Epoch 1, it 7: loss 0.857\n",
      "Epoch 1, it 8: loss 0.733\n",
      "Epoch 1, it 9: loss 0.741\n",
      "Epoch 1, it 10: loss 0.608\n",
      "Epoch 1, it 100: loss 0.815\n",
      "Epoch 1, it 200: loss 0.595\n",
      "Epoch 1, it 300: loss 0.772\n",
      "Epoch 1, it 400: loss 0.738\n",
      "Epoch 1, it 500: loss 0.628\n",
      "Epoch 1, it 600: loss 0.734\n",
      "Epoch 1, it 700: loss 0.624\n",
      "\n",
      "Accuracy on test set: 0.784805178642273\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Same using nn module\n",
    "\"\"\"\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10).double() / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "        self.lr = 0.01\n",
    "        self.bs = 64\n",
    "        self.epochs = 2\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return log_softmax(xb @ self.weights + self.bias)\n",
    "\n",
    "    \n",
    "    def fit(self, x_train, y_train):\n",
    "        n, c = x_train.shape\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range((n - 1) // self.bs + 1):\n",
    "                start_i = i * self.bs\n",
    "                end_i = start_i + self.bs\n",
    "                xb = x_train[start_i:end_i]\n",
    "                yb = y_train[start_i:end_i]\n",
    "                pred = self.forward(xb)\n",
    "                loss = nll_loss(pred, yb)\n",
    "                if i % 100 == 0 or i <= 10:\n",
    "                    print(f\"Epoch {epoch}, it {i}: loss {round(loss.item(), 3)}\")\n",
    "\n",
    "                loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    for p in model.parameters():\n",
    "                        p -= p.grad * self.lr\n",
    "                    model.zero_grad()\n",
    "        print(f\"\\nAccuracy on test set: {accuracy(model(x_test), y_test)}\")\n",
    "\n",
    "model = Mnist_Logistic()\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e6f96f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, it 0: loss 2.315\n",
      "Epoch 0, it 100: loss 0.529\n",
      "Epoch 0, it 200: loss 0.719\n",
      "Epoch 0, it 300: loss 0.588\n",
      "\tValidation loss after epoch: 0.574\n",
      "Epoch 1, it 0: loss 0.55\n",
      "Epoch 1, it 100: loss 0.372\n",
      "Epoch 1, it 200: loss 0.63\n",
      "Epoch 1, it 300: loss 0.514\n",
      "\tValidation loss after epoch: 0.521\n",
      "Epoch 2, it 0: loss 0.487\n",
      "Epoch 2, it 100: loss 0.343\n",
      "Epoch 2, it 200: loss 0.599\n",
      "Epoch 2, it 300: loss 0.48\n",
      "\tValidation loss after epoch: 0.498\n",
      "Epoch 3, it 0: loss 0.454\n",
      "Epoch 3, it 100: loss 0.328\n",
      "Epoch 3, it 200: loss 0.582\n",
      "Epoch 3, it 300: loss 0.458\n",
      "\tValidation loss after epoch: 0.484\n",
      "Epoch 4, it 0: loss 0.432\n",
      "Epoch 4, it 100: loss 0.319\n",
      "Epoch 4, it 200: loss 0.571\n",
      "Epoch 4, it 300: loss 0.443\n",
      "\tValidation loss after epoch: 0.474\n",
      "Epoch 5, it 0: loss 0.416\n",
      "Epoch 5, it 100: loss 0.312\n",
      "Epoch 5, it 200: loss 0.562\n",
      "Epoch 5, it 300: loss 0.432\n",
      "\tValidation loss after epoch: 0.467\n",
      "Epoch 6, it 0: loss 0.404\n",
      "Epoch 6, it 100: loss 0.306\n",
      "Epoch 6, it 200: loss 0.555\n",
      "Epoch 6, it 300: loss 0.422\n",
      "\tValidation loss after epoch: 0.462\n",
      "Epoch 7, it 0: loss 0.395\n",
      "Epoch 7, it 100: loss 0.301\n",
      "Epoch 7, it 200: loss 0.55\n",
      "Epoch 7, it 300: loss 0.414\n",
      "\tValidation loss after epoch: 0.458\n",
      "Epoch 8, it 0: loss 0.387\n",
      "Epoch 8, it 100: loss 0.297\n",
      "Epoch 8, it 200: loss 0.545\n",
      "Epoch 8, it 300: loss 0.408\n",
      "\tValidation loss after epoch: 0.454\n",
      "Epoch 9, it 0: loss 0.381\n",
      "Epoch 9, it 100: loss 0.294\n",
      "Epoch 9, it 200: loss 0.54\n",
      "Epoch 9, it 300: loss 0.402\n",
      "\tValidation loss after epoch: 0.451\n",
      "\n",
      "Accuracy on full test set: 0.8482251167297363\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Using nn module with automations and fancy stuff\n",
    "\"\"\"\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr = 0.01\n",
    "        self.bs = 128\n",
    "        self.epochs = 10\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "        self.opt = optim.SGD(self.parameters(), lr=self.lr, momentum=.9)\n",
    "        self.loss = F.cross_entropy\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return log_softmax(self.lin(xb.float()))\n",
    "    \n",
    "    def fit(self, x_train, y_train, x_test, y_test):\n",
    "        train_ds = TensorDataset(x_train, y_train)\n",
    "        train_dl = DataLoader(train_ds, batch_size=self.bs)\n",
    "        test_ds = TensorDataset(x_test, y_test)\n",
    "        test_dl = DataLoader(test_ds, batch_size=bs * 2)\n",
    "        n, c = x_train.shape\n",
    "        for epoch in range(self.epochs):\n",
    "            model.train() # flags training mode needed for certain layers like bathc normalization and dropout\n",
    "            for i, (xb, yb) in enumerate(train_dl):\n",
    "                loss = self.loss(self.forward(xb), yb) # TODO: equivalent here to using nll_loss, why exactly?\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"Epoch {epoch}, it {i}: loss {round(loss.item(), 3)}\")\n",
    "\n",
    "                loss.backward()\n",
    "                self.opt.step()\n",
    "                self.opt.zero_grad()\n",
    "            model.eval() # flags test mode\n",
    "            with torch.no_grad():\n",
    "                valid_loss = sum(self.loss(self.forward(xb), yb) for xb, yb in test_dl) / len(test_dl)\n",
    "                print(f\"\\tValidation loss after epoch: {round(valid_loss.item(), 3)}\")\n",
    "        \n",
    "        print(f\"\\nAccuracy on full test set: {accuracy(model(x_test), y_test)}\")\n",
    "\n",
    "model = Mnist_Logistic()\n",
    "model.fit(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b1e340c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, it 0: loss 2.313\n",
      "Epoch 0, it 100: loss 1.954\n",
      "Epoch 0, it 200: loss 1.769\n",
      "Epoch 0, it 300: loss 1.676\n",
      "\tValidation loss after epoch: 1.552\n",
      "Epoch 1, it 0: loss 1.679\n",
      "Epoch 1, it 100: loss 1.412\n",
      "Epoch 1, it 200: loss 1.595\n",
      "Epoch 1, it 300: loss 1.578\n",
      "\tValidation loss after epoch: 1.508\n",
      "Epoch 2, it 0: loss 1.64\n",
      "Epoch 2, it 100: loss 1.399\n",
      "Epoch 2, it 200: loss 1.514\n",
      "Epoch 2, it 300: loss 1.565\n",
      "\tValidation loss after epoch: 1.488\n",
      "Epoch 3, it 0: loss 1.612\n",
      "Epoch 3, it 100: loss 1.395\n",
      "Epoch 3, it 200: loss 1.484\n",
      "Epoch 3, it 300: loss 1.545\n",
      "\tValidation loss after epoch: 1.47\n",
      "Epoch 4, it 0: loss 1.588\n",
      "Epoch 4, it 100: loss 1.363\n",
      "Epoch 4, it 200: loss 1.456\n",
      "Epoch 4, it 300: loss 1.531\n",
      "\tValidation loss after epoch: 1.466\n",
      "Epoch 5, it 0: loss 1.577\n",
      "Epoch 5, it 100: loss 1.355\n",
      "Epoch 5, it 200: loss 1.436\n",
      "Epoch 5, it 300: loss 1.519\n",
      "\tValidation loss after epoch: 1.456\n",
      "Epoch 6, it 0: loss 1.566\n",
      "Epoch 6, it 100: loss 1.347\n",
      "Epoch 6, it 200: loss 1.428\n",
      "Epoch 6, it 300: loss 1.508\n",
      "\tValidation loss after epoch: 1.446\n",
      "Epoch 7, it 0: loss 1.557\n",
      "Epoch 7, it 100: loss 1.341\n",
      "Epoch 7, it 200: loss 1.412\n",
      "Epoch 7, it 300: loss 1.498\n",
      "\tValidation loss after epoch: 1.446\n",
      "Epoch 8, it 0: loss 1.559\n",
      "Epoch 8, it 100: loss 1.328\n",
      "Epoch 8, it 200: loss 1.402\n",
      "Epoch 8, it 300: loss 1.493\n",
      "\tValidation loss after epoch: 1.437\n",
      "Epoch 9, it 0: loss 1.557\n",
      "Epoch 9, it 100: loss 1.325\n",
      "Epoch 9, it 200: loss 1.395\n",
      "Epoch 9, it 300: loss 1.491\n",
      "\tValidation loss after epoch: 1.436\n",
      "\n",
      "Accuracy on full test set: 0.45930737257003784\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fully Connected Dense NN\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Using nn module with automations and fancy stuff\n",
    "\"\"\"\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr = 0.01\n",
    "        self.bs = 128\n",
    "        self.epochs = 10\n",
    "        self.lin1 = nn.Linear(784, 256)\n",
    "        self.lin2 = nn.Linear(256, 10)\n",
    "        self.lin3 = nn.Linear(10, 10)\n",
    "        self.opt = optim.SGD(self.parameters(), lr=self.lr, momentum=.9)\n",
    "        self.loss = F.cross_entropy\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = F.relu(self.lin1(xb.float()))\n",
    "        xb = F.relu(self.lin2(xb))\n",
    "        xb = F.relu(self.lin3(xb))\n",
    "        return log_softmax(xb)\n",
    "    \n",
    "    def fit(self, x_train, y_train, x_test, y_test):\n",
    "        train_ds = TensorDataset(x_train, y_train)\n",
    "        train_dl = DataLoader(train_ds, batch_size=self.bs)\n",
    "        test_ds = TensorDataset(x_test, y_test)\n",
    "        test_dl = DataLoader(test_ds, batch_size=bs * 2)\n",
    "        n, c = x_train.shape\n",
    "        for epoch in range(self.epochs):\n",
    "            model.train() # flags training mode needed for certain layers like bathc normalization and dropout\n",
    "            for i, (xb, yb) in enumerate(train_dl):\n",
    "                loss = self.loss(self.forward(xb), yb) # TODO: equivalent here to using nll_loss, why exactly?\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"Epoch {epoch}, it {i}: loss {round(loss.item(), 3)}\")\n",
    "\n",
    "                loss.backward()\n",
    "                self.opt.step()\n",
    "                self.opt.zero_grad()\n",
    "            model.eval() # flags test mode\n",
    "            with torch.no_grad():\n",
    "                valid_loss = sum(self.loss(self.forward(xb), yb) for xb, yb in test_dl) / len(test_dl)\n",
    "                print(f\"\\tValidation loss after epoch: {round(valid_loss.item(), 3)}\")\n",
    "        \n",
    "        print(f\"\\nAccuracy on full test set: {accuracy(model(x_test), y_test)}\")\n",
    "\n",
    "model = Mnist_Logistic()\n",
    "model.fit(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8224b791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, it 0: loss 2.309\n",
      "Epoch 0, it 100: loss 2.302\n",
      "Epoch 0, it 200: loss 2.28\n",
      "Epoch 0, it 300: loss 2.169\n",
      "Epoch 0, it 400: loss 1.496\n",
      "Epoch 0, it 500: loss 1.353\n",
      "Epoch 0, it 600: loss 1.384\n",
      "Epoch 0, it 700: loss 0.903\n",
      "\tValidation loss after epoch: 1.088\n",
      "Epoch 1, it 0: loss 1.265\n",
      "Epoch 1, it 100: loss 1.098\n",
      "Epoch 1, it 200: loss 0.774\n",
      "Epoch 1, it 300: loss 1.015\n",
      "Epoch 1, it 400: loss 0.995\n",
      "Epoch 1, it 500: loss 0.805\n",
      "Epoch 1, it 600: loss 0.924\n",
      "Epoch 1, it 700: loss 0.548\n",
      "\tValidation loss after epoch: 0.806\n",
      "Epoch 2, it 0: loss 1.037\n",
      "Epoch 2, it 100: loss 0.936\n",
      "Epoch 2, it 200: loss 0.527\n",
      "Epoch 2, it 300: loss 0.783\n",
      "Epoch 2, it 400: loss 0.771\n",
      "Epoch 2, it 500: loss 0.579\n",
      "Epoch 2, it 600: loss 0.887\n",
      "Epoch 2, it 700: loss 0.427\n",
      "\tValidation loss after epoch: 0.709\n",
      "Epoch 3, it 0: loss 1.032\n",
      "Epoch 3, it 100: loss 0.745\n",
      "Epoch 3, it 200: loss 0.48\n",
      "Epoch 3, it 300: loss 0.766\n",
      "Epoch 3, it 400: loss 0.704\n",
      "Epoch 3, it 500: loss 0.51\n",
      "Epoch 3, it 600: loss 0.85\n",
      "Epoch 3, it 700: loss 0.389\n",
      "\tValidation loss after epoch: 0.644\n",
      "Epoch 4, it 0: loss 0.995\n",
      "Epoch 4, it 100: loss 0.683\n",
      "Epoch 4, it 200: loss 0.464\n",
      "Epoch 4, it 300: loss 0.74\n",
      "Epoch 4, it 400: loss 0.669\n",
      "Epoch 4, it 500: loss 0.5\n",
      "Epoch 4, it 600: loss 0.823\n",
      "Epoch 4, it 700: loss 0.356\n",
      "\tValidation loss after epoch: 0.603\n",
      "Epoch 5, it 0: loss 0.95\n",
      "Epoch 5, it 100: loss 0.643\n",
      "Epoch 5, it 200: loss 0.452\n",
      "Epoch 5, it 300: loss 0.706\n",
      "Epoch 5, it 400: loss 0.651\n",
      "Epoch 5, it 500: loss 0.493\n",
      "Epoch 5, it 600: loss 0.809\n",
      "Epoch 5, it 700: loss 0.335\n",
      "\tValidation loss after epoch: 0.565\n",
      "Epoch 6, it 0: loss 0.89\n",
      "Epoch 6, it 100: loss 0.598\n",
      "Epoch 6, it 200: loss 0.415\n",
      "Epoch 6, it 300: loss 0.669\n",
      "Epoch 6, it 400: loss 0.641\n",
      "Epoch 6, it 500: loss 0.488\n",
      "Epoch 6, it 600: loss 0.78\n",
      "Epoch 6, it 700: loss 0.32\n",
      "\tValidation loss after epoch: 0.525\n",
      "Epoch 7, it 0: loss 0.787\n",
      "Epoch 7, it 100: loss 0.566\n",
      "Epoch 7, it 200: loss 0.391\n",
      "Epoch 7, it 300: loss 0.648\n",
      "Epoch 7, it 400: loss 0.623\n",
      "Epoch 7, it 500: loss 0.481\n",
      "Epoch 7, it 600: loss 0.754\n",
      "Epoch 7, it 700: loss 0.295\n",
      "\tValidation loss after epoch: 0.508\n",
      "Epoch 8, it 0: loss 0.732\n",
      "Epoch 8, it 100: loss 0.554\n",
      "Epoch 8, it 200: loss 0.38\n",
      "Epoch 8, it 300: loss 0.642\n",
      "Epoch 8, it 400: loss 0.595\n",
      "Epoch 8, it 500: loss 0.473\n",
      "Epoch 8, it 600: loss 0.736\n",
      "Epoch 8, it 700: loss 0.279\n",
      "\tValidation loss after epoch: 0.499\n",
      "Epoch 9, it 0: loss 0.688\n",
      "Epoch 9, it 100: loss 0.534\n",
      "Epoch 9, it 200: loss 0.378\n",
      "Epoch 9, it 300: loss 0.632\n",
      "Epoch 9, it 400: loss 0.577\n",
      "Epoch 9, it 500: loss 0.458\n",
      "Epoch 9, it 600: loss 0.707\n",
      "Epoch 9, it 700: loss 0.268\n",
      "\tValidation loss after epoch: 0.492\n",
      "\n",
      "Accuracy on full test set: 0.8269264101982117\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ConvNN\n",
    "\"\"\"\n",
    "\n",
    "class Mnist_ConvNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr = 0.01\n",
    "        self.bs = 64\n",
    "        self.epochs = 10\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.opt = optim.SGD(self.parameters(), lr=self.lr, momentum=.9)\n",
    "        self.loss = F.cross_entropy\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        \n",
    "        xb = xb.float().view(-1, 1, 28, 28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        return log_softmax(xb.view(-1, xb.size(1)))\n",
    "    \n",
    "        #return log_softmax(self.lin(xb.float()))\n",
    "    \n",
    "    def fit(self, x_train, y_train, x_test, y_test):\n",
    "        train_ds = TensorDataset(x_train, y_train)\n",
    "        train_dl = DataLoader(train_ds, batch_size=self.bs)\n",
    "        test_ds = TensorDataset(x_test, y_test)\n",
    "        test_dl = DataLoader(test_ds, batch_size=bs * 2)\n",
    "        n, c = x_train.shape\n",
    "        for epoch in range(self.epochs):\n",
    "            model.train() # flags training mode needed for certain layers like bathc normalization and dropout\n",
    "            for i, (xb, yb) in enumerate(train_dl):\n",
    "                loss = self.loss(self.forward(xb), yb) # TODO: equivalent here to using nll_loss, why exactly?\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"Epoch {epoch}, it {i}: loss {round(loss.item(), 3)}\")\n",
    "\n",
    "                loss.backward()\n",
    "                self.opt.step()\n",
    "                self.opt.zero_grad()\n",
    "            model.eval() # flags test mode\n",
    "            with torch.no_grad():\n",
    "                valid_loss = sum(self.loss(self.forward(xb), yb) for xb, yb in test_dl) / len(test_dl)\n",
    "                print(f\"\\tValidation loss after epoch: {round(valid_loss.item(), 3)}\")\n",
    "        \n",
    "        print(f\"\\nAccuracy on full test set: {accuracy(model(x_test), y_test)}\")\n",
    "\n",
    "model = Mnist_ConvNN()\n",
    "model.fit(x_train, y_train, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
